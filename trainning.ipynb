{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe98b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional,GlobalAveragePooling1D, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ab571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Các biến cấu hình\n",
    "DATA_PATH        = 'Data'                # thư mục gốc chứa các folder action\n",
    "LABEL_MAP_PATH   = 'Logs/label_map.json'\n",
    "BATCH_SIZE       = 32\n",
    "AUTOTUNE         = tf.data.AUTOTUNE\n",
    "VAL_SPLIT        = 0.1\n",
    "TEST_SPLIT       = 0.1\n",
    "# 2. Load label_map từ JSON\n",
    "with open(LABEL_MAP_PATH, 'r', encoding='utf-8') as f:\n",
    "    label_map = json.load(f)            # ví dụ: {\"địa chỉ\": 0, \"miến điện\": 1, ...}\n",
    "\n",
    "# 3. Tạo danh sách tất cả các file .npz\n",
    "#    Data structure: Data/<action_name>/*.npz\n",
    "file_pattern = os.path.join(DATA_PATH, '**', '*.npz')\n",
    "all_files = glob.glob(file_pattern, recursive=True)\n",
    "print(f\"Found {len(all_files)} samples.\")\n",
    "\n",
    "train_files, temp_files = train_test_split(\n",
    "    all_files,\n",
    "    test_size=VAL_SPLIT + TEST_SPLIT,  # ví dụ: 0.2 + 0.1 = 0.3\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=[os.path.basename(p).split('.')[0] for p in all_files]\n",
    ")\n",
    "\n",
    "val_files, test_files = train_test_split(\n",
    "    temp_files,\n",
    "    test_size=TEST_SPLIT / (VAL_SPLIT + TEST_SPLIT),  # ví dụ: 0.1 / 0.3 = 1/3\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=[os.path.basename(p).split('.')[0] for p in temp_files]\n",
    ")\n",
    "\n",
    "print(f\"  Train samples: {len(train_files)}\")\n",
    "print(f\"    Val samples: {len(val_files)}\")\n",
    "print(f\"   Test samples: {len(test_files)}\")\n",
    "\n",
    "# 4. Hàm parse mỗi file .npz\n",
    "def _load_npz(path):\n",
    "    # path: scalar tf.string tensor\n",
    "    npz_path = path.decode('utf-8')\n",
    "    data = np.load(npz_path)\n",
    "    seq   = data['sequence'].astype(np.float32)   # (60,126)\n",
    "    lbl   = np.int32(data['label'])\n",
    "    return seq, lbl\n",
    "\n",
    "def parse_fn(path):\n",
    "    seq, lbl = tf.numpy_function(\n",
    "        func=_load_npz,\n",
    "        inp=[path],\n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    # set shape để TF biết kích thước cố định\n",
    "    seq.set_shape([60, 201])\n",
    "    lbl.set_shape([])\n",
    "    return seq, lbl\n",
    "def make_dataset(file_list, shuffle=False, repeat=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(file_list), reshuffle_each_iteration=True)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.map(parse_fn, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# 6. Tạo train_ds & val_ds\n",
    "train_ds = make_dataset(train_files, shuffle=True, repeat=True)\n",
    "val_ds   = make_dataset(val_files, shuffle=False, repeat=False)\n",
    "test_ds  = make_dataset(test_files, shuffle=False, repeat=False)\n",
    "\n",
    "# 7. Compute steps\n",
    "steps_per_epoch = len(train_files) // BATCH_SIZE\n",
    "validation_steps = len(val_files) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(60, 201))\n",
    "\n",
    "# Khối LSTM thứ nhất\n",
    "x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.3))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Khối LSTM thứ hai\n",
    "x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Khối LSTM thứ ba\n",
    "x = Bidirectional(LSTM(256, dropout=0.3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Các lớp Dense\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Lớp đầu ra\n",
    "outputs = Dense(2764, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "# Biên dịch mô hình\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tạo thư mục lưu checkpoint (nếu chưa có)\n",
    "checkpoint_dir = 'Models/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'final_model.keras')\n",
    "\n",
    "# 2. Khởi tạo callbacks\n",
    "callbacks = [\n",
    "    # Lưu mô hình với val_loss thấp nhất\n",
    "    ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,  # lưu cả kiến trúc + weights\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Dừng training nếu 5 epoch liên tiếp không cải thiện val_loss\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca557bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
